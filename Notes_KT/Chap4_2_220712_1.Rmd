https://github.com/tsunodak/cmbd-book/blob/master/chapter4/qlearning_groupdata.R#L44
が参考になる

```{r}
# ----------------------------------------------------- #
# Q学習モデルのシミュレーションにより選択データを生成し，
# そのデータから最尤推定をする。で
# ----------------------------------------------------- #

# メモリ，グラフのクリア
rm(list=ls())
graphics.off()

# 描画のためのライブラリ読み込み
library(tidyverse)
library(gridExtra)
library(plotly)
library(rgl)
library(scatterplot3d)

# 乱数のシードの設定 
set.seed(141)
```

```{r}
func_simu_qlearn <- function(T, alpha, beta, pr)
{
  # Q 値の初期化( 選択肢の数x T)
  Q <- matrix(numeric(2 * T), nrow = 2, ncol = T)
  
  c <- numeric(T)   # choice
  r <- numeric(T)   # reward
  pA <- numeric(T)  # choice probability
  
  
  
  for (t in 1:T) {
    # ソフトマックスで選択肢A の選択確率を決定する
    pA[t] <- 1 / (1 + exp(-beta * (Q[1, t] - Q[2, t])))
    
    if (runif(1, 0, 1) < pA[t]) {
      # 0~1の実数の生成　runif
      # 引数 n は試行の回数、min は出力する乱数の最小値、max は出力する乱数の最大値である。
      
      # Aを選択
      c[t] <- 1
      r[t] <- as.numeric(runif(1, 0, 1) < pr[1]) # e.g.: 0.434 < 0.7の場合
      # as.numericで0 or 1にする
    } else {
      # Bを選択
      c[t] <- 2
      r[t] <- as.numeric(runif(1, 0, 1) < pr[2]) # e.g.: 0.434 < 0.3の場合
    }
    
    # 行動価値の更新
    if (t < T) {
      Q[c[t], t + 1] <- Q[c[t], t] + alpha * (r[t] - Q[c[t], t])
      
      # 選択肢していない行動の価値はそのままの値を次の時刻に引き継ぐ。
      # 3-c でc=1 なら2, c=2 なら1, というように
      # 逆側の選択肢のインデックスが求まる。
      Q[3 - c[t], t + 1] <- Q[3 - c[t], t]
      
      # Qの構造
      # 1行目　Aの選択価値
      # 2行目　Bの選択価値
    }
  }
  
  NumA <- sum(c == 1)
  PercentA <- sum(c == 1) / T * 100
#  return(PercentA)
  return(list(Choice = c, ProbA = PercentA))
}

```
1人分のデータのシミュレーション
```{r}
T     <- 200     # 試行数
alpha <- 0.3     # 学習率
beta  <- 2.0      # 逆温度
pr    <- c(0.7, 0.3) # それぞれの選択肢の報酬確率
Val   <- func_simu_qlearn(T, alpha, beta, pr)

print(Val)
print(sprintf("Percent of A choices: %.2f %%", Val[2]))
```
複数人分のシミュレーションデータの生成
```{r}
# Initial values of subjects
T                     <- 200  # Numer of trials
N                     <- 30   # Number of subjects
initparam_alpha_all <- rnorm(N, mean=0.5, sd=0.1)
initparam_beta_all <- rnorm(N, mean=4,    sd=1)
initparam_alpha_all[initparam_alpha_all < 0.01] <- 0.01 # rnormだと0~1に収まらない値が生成される可能性あ
initparam_alpha_all[initparam_alpha_all > 0.99] <- 0.99
initparam_beta_all[initparam_beta_all   < 0.0]  <- 0.01
pr                    <- c(0.7, 0.3) # それぞれの選択肢の報酬確率
Rep                   <- 1

# Make simulation data
Ind_pA_all    <- as.numeric(length(initparam_alpha_all) * length(initparam_beta_all))
pA_all        <- numeric(Ind_pA_all)
sim_alpha_all <- numeric(length(initparam_alpha_all)*length(initparam_beta_all)*Rep) 
sim_beta_all  <- numeric(length(initparam_alpha_all)*length(initparam_beta_all)*Rep) 
sim_pA_all    <- numeric(length(initparam_alpha_all)*length(initparam_beta_all)*Rep) 
iRepLoop <- 1
for (iN in 1:N) {
  alpha_tmp  <- initparam_alpha_all[iN]
    beta_tmp <- initparam_beta_all[iN]
      Val    <- func_simu_qlearn(T, alpha_tmp, beta_tmp, pr)
      
      sim_alpha_all[iRepLoop] <- alpha_tmp
      sim_beta_all[iRepLoop]  <- beta_tmp
      sim_pA_all[iRepLoop]    <- as.numeric(Val$ProbA)
        
      iRepLoop <- iRepLoop + 1
}

# 各組合せ条件を１つのラベルとなるように新たなラベルを作成　100条件
Char_alpha <- as.character(sim_alpha_all)
Char_beta  <- as.character(sim_beta_all)
Char_param <- paste(Char_alpha , Char_beta , sep = "_") # 
# make dataframe
Data_sim   <- data.frame(sim_alpha_all, sim_beta_all, sim_pA_all, Char_param)
```
simulation dataの描画
```{r}
summary(Data_sim)
# plot
hist(initparam_alpha_all)
hist(initparam_beta_all)
ggplot(Data_sim) +  geom_point(aes(x = sim_alpha_all, y= sim_pA_all, color = sim_beta_all))
plot_ly(x=~Data_sim$sim_alpha_all, y=~Data_sim$sim_beta_all, z=~Data_sim$sim_pA_all, color = sim_beta_all) # 3d plot
# z=~　チルダをつけることで変数名を表示してくれる
# Fig + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
